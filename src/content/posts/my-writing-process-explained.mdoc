---
title: My Writing Process- EXPLAINED
status: published
featured: false
publishedDate: 2025-11-08T13:16:00.000Z
category: ai-implementation
tags:
  - prompt-engineering
  - workflow-design
  - specialization
  - ai-implementation
  - professional-development
  - reflection
excerpt: >-
  Discover how AI and human judgment work together in a specialized, multi-stage
  workflow that actually maintains quality and authentic voice.
takeaways:
  - title: Specialize your assistants
    text: >-
      One general-purpose assistant produces mediocre results across all tasks.
      Build specialized assistants optimized for one job each: Claude for
      complex reasoning, Gemini for fast extraction, Copilot for visual
      generation. Match the tool to the job, not the other way around.
  - title: Know your division of labor
    text: >-
      AI excels at pattern recognition and initial drafting. You own the ideas
      (from real experience), the quality standards, and the fact-checking. The
      author spends 45-90 minutes in heavy editing—adding examples, fixing
      voice, ensuring authenticity. That’s where the actual work happens. You’re
      the general contractor; AI is a specialized subcontractor.
  - title: Build stages that demand human judgment
    text: >-
      Don’t just review work at the end. Structure your workflow so humans
      actively shape the work throughout: ideation (you) → draft (AI) → heavy
      editing (you) → review (AI) → extraction (you) → generation (AI) → final
      check (you). This forces engagement at every stage, not just approval at
      the finish line.
  - title: Separate generation from evaluation
    text: >-
      When reviewing draft work, start a fresh conversation with your assistant.
      This prevents confirmation bias. The assistant isn’t defending its
      original output—it’s genuinely evaluating your edited version. Fresh
      context enables honest, actionable feedback.
cover: ../../assets/blogIMG/my-writing-process-explained/cover.jpeg
coverAlt: >-
  A figure in dark business attire pulls back a teal curtain, revealing an
  interconnected system of gears and cogs in yellow, coral, and red. Thin lines
  connect the gears in a mechanical workflow pattern against a dark blue
  background.
---
# How I Create Content

You clicked through because I disclosed AI assistance in that blog post. Good.

I think transparency matters, especially for someone teaching others to work with AI. If I’m writing about prompt engineering and UX design while hiding how I actually work, that’s hypocritical.

So here’s the full breakdown: every tool, every assistant, every decision in my content creation workflow.

## Why I Use AI Tools

I use AI for three reasons, each building on the others.

**First, efficiency, but not the kind you might think.** AI doesn’t make me faster at writing. It makes me faster at starting. The blank page problem isn’t about typing speed, it’s about initial momentum. Once I have a rough draft, I can focus my energy on the work I’m actually good at: refining ideas, adding depth from my experience, catching nuance.

**Second, specialization.** Drafting requires different capabilities than evaluation, which requires different capabilities than visual conceptualization. A single “do everything” assistant produces mediocre results across all tasks. I learned this the hard way after months of using ChatGPT for everything. Now I build specialized assistants optimized for specific parts of my workflow.

**Third, practice.** I work with AI daily and develop prompts for UX professionals. UX Prompt Lab is about working effectively with AI. Using these tools is practicing what I preach. The posts you read here reflect my real experiences and insights from working with AI in tech and UX, refined through a deliberate, multi-stage editorial process.

## My Writing Workflow

**1. Ideation & Planning (Me)**\
I identify topics based on my work experience and outline the key points I want to cover. This foundation is 100% human.

**2. First Draft (AI-Assisted)**\
I use a specialized drafting assistant that analyzes my past writing to generate an initial draft from my outline.

**3. Heavy Editing (Me)**\
Multiple rounds of rewriting, fact-checking, adding personal examples, and refining the voice. This is where most of the work happens.

**4. Review (AI-Assisted)**\
A fresh evaluation of my edited draft against quality guidelines to identify what’s working and what could be stronger.

**5. Visual Planning (AI-Assisted)**\
Extract the main narrative beats and themes from the finished post to prepare for image creation.

**6. Image Generation (AI-Assisted)**\
Generate branded images using a modular template system that adapts to different content needs.

## The Actual Division of Labor

Here’s how the work breaks down.

I own three things completely: **the ideas** (which come from my work experiences with AI in tech and UX), **the quality standards** (I decide what’s good enough to publish), and **the fact-checking** (AI can’t verify its own claims).

The AI owns **pattern recognition**. It’s better than me at identifying structural parallels across my past writing and maintaining consistent voice across a 2000-word draft. It analyzes how I think through problems, how I structure arguments, how I move from personal experience to broader insight.

Think of it like this: I’m the architect and general contractor. The AI is a specialized subcontractor who’s really good at initial framing and can work fast once I’ve approved the blueprint. But I’m reviewing every joint, testing every load-bearing wall, and signing off on the final inspection.

If there’s an error in a post, it’s my error. If there’s valuable insight, it came from my knowledge and experience. The tools may be sophisticated, but the standard doesn’t change: **I only publish content I’m proud to attach my name to.**

## My Commitment to You

Every post represents my authentic thoughts and expertise from working with AI in tech and UX. I take full responsibility for the content, just as I would if I’d typed every word manually. The tools may be sophisticated, but the standard doesn’t change: I only publish content I’m proud to attach my name to.

---

## For Those Curious About My Full Detailed Workflow

If you want to understand exactly how I work with AI tools (including the specialized assistants I’ve built, my multi-platform approach, and detailed prompt engineering process), here’s the complete breakdown:

### **Meta-Level: Building My Assistant Ecosystem**

Before I write any blog posts, I build the specialized assistants that power my workflow.

**Tool: LLM Prompt Engineer**

- Platform: Claude Sonnet 4.5 (iOS app)
- Purpose: Designs the instructions for all my other assistants
- Time investment: 1-2 hours per new assistant

Think of a system prompt as the permanent instructions that define an AI assistant’s behavior, like a job description that persists across every conversation. Instead of telling ChatGPT what to do every single time, you write one comprehensive instruction set that shapes how it responds to everything.

I use a specialized assistant to build these instructions for BlogBuilder, Narrative Analyzer, and IMG Generator. Rather than iterating blindly on prompt text (which is how most people approach this and waste hours), this assistant walks me through a structured two-phase process:

**Phase 1: Systematic Clarification**\
The assistant asks focused questions one at a time to understand requirements, constraints, and success criteria. It stops after each question to prevent information overload. (This “one question at a time, then STOP” rule is essential. Without it, Claude tends to ask multiple questions simultaneously, which fragments the whole process.)

**Phase 2: Structured Generation**\
Once requirements are clear, it generates production-ready prompts following a consistent structure: role and constraints, examples and knowledge, specific task, format enforcement.

**Why this works:** The phase-gated structure prevents the most common failure mode in prompt engineering: building solutions before fully understanding the problem. I’ve found this produces far more robust assistants than ad-hoc prompt writing.

---

### **Step 1: Concept & Planning (Human)**

*Time: 15-30 minutes*

Everything starts with me. I identify topics based on:

- Problems I’m solving at work
- Interesting patterns I’m noticing with AI tools
- Questions people ask me about AI in UX
- Techniques I’ve tested and validated

I create a rough outline with key points, arguments, and any specific examples I want to include. This is 100% human thinking. No AI has context about my work experiences or insights.

---

### **Step 2: First Draft Generation with BlogBuilder**

*Time: 5-10 minutes of input, instant generation*

**Tool: BlogBuilder (Generation Mode)**

- Platform: Claude Sonnet 4.5 (iOS app)
- Project files: Past writing examples + creative non-fiction knowledge base
- Input: Topic and rough outline
- Output: Draft 1

BlogBuilder is my primary drafting partner. I built it to handle two different jobs: sometimes I need help starting from a blank page, other times I need someone to tell me what’s wrong with my draft. Those require different mindsets, so the assistant switches modes depending on what I need.

**How it works:**\
When I send my topic and outline, BlogBuilder:

1. Asks 3-4 focused questions about my angle, audience, and constraints (which I can skip if I already know what I want)
1. Searches my project files for past writing examples
1. Analyzes my voice by examining cognitive stance, emotional posture, and structural patterns (not just surface-level style like “uses short paragraphs”)
1. Generates first draft that mimics how I think through ideas

**Key capabilities:**

- **Adaptive questioning protocol** that I can skip if I already know what I want
- **Creative consultation mode** for when I’m unclear on my angle. It proposes unexpected directions and challenges generic framing. (This emerged from testing. I realized I often don’t know what angle I want until someone pushes back on my first instinct.)
- **Voice learning from examples** rather than following abstract style rules

**Why Claude Sonnet 4.5:** Strong reasoning capabilities, large context window for analyzing multiple writing examples, and reliable adherence to complex instructions make it ideal for this task.

**Why this approach works:** The voice-learning protocol is intentionally anti-formulaic. Instead of “write like this checklist,” it analyzes example posts for cognitive patterns and emotional posture. This produces more authentic-sounding drafts than rule-following ever did.

---

### **Step 3: Heavy Editing (Human)**

*Time: 45-90 minutes*

This is where the real work happens. I go through multiple passes:

**First Pass: Voice & Authenticity**

- Rewrite sections that sound generic or “AI-like”
- Add specific examples from my actual work
- Insert personal observations and opinions
- Remove corporate-speak and buzzwords
- Ensure it sounds like me talking to a colleague

**Second Pass: Accuracy & Depth**

- Fact-check technical claims
- Add context the AI couldn’t know
- Verify that recommendations match real-world usage
- Ensure any workflows or examples are practical and tested
- Add nuance based on my professional experience

**Third Pass: Structure & Flow**

- Rearrange sections for better narrative
- Strengthen transitions between ideas
- Cut redundant content
- Ensure each section adds unique value
- Check that the post delivers on its promise

By this point, the post has been substantially reworked from the initial draft. The structure might be similar, but the voice, examples, and specific insights are entirely mine.

---

### **Step 4: Draft Review with BlogBuilder**

*Time: 10-15 minutes*

**Tool: BlogBuilder (Review Mode)**

- Platform: Claude Sonnet 4.5 (iOS app, fresh chat with same system prompt)
- Input: Heavily edited draft
- Output: Structured evaluation feedback

After my heavy editing, I start a **fresh conversation** with BlogBuilder and switch it to review mode. This is critical. I want the assistant to evaluate my work objectively, not remember the original draft it generated.

**How it works:**\
BlogBuilder:

1. Searches my project files for my writing guidelines document
1. Analyzes the draft against established quality criteria
1. Provides structured feedback in three categories:

- What’s working well
- Opportunities to strengthen
- Alignment check with my style and standards

**Why this approach works:** Using the same assistant in a different mode maintains consistency. It understands my voice from the same examples it used for generation, but now applies that understanding critically. The fresh chat prevents confirmation bias. The assistant isn’t defending its original draft, it’s genuinely evaluating my edited version.

---

### **Step 5: Narrative Extraction with Narrative Analyzer**

*Time: 5 minutes*

**Tool: Narrative Analyzer**

- Platform: Google Gemini 2.5 Flash (Gems feature)
- Input: Finished blog post
- Output: First-person narrative beats

Once the post is finalized, I need to identify the visual moments that will become images. This turned out to be trickier than I expected.

Here’s the problem: if I ask an AI to “analyze this post and suggest images,” it writes from an outside perspective. “The author discusses prompt drift” or “This section explains context windows.” That third-person analytical distance creates abstract summaries that don’t translate well to visual prompts.

So I configured this assistant to adopt my voice and write as if I’m reflecting on my own post structure. First-person perspective forces the analysis to be concrete and visual rather than meta-analytical.

**How it works:**\
The assistant:

1. Maps the post to narrative structure (theme, context, rising action, climax, resolution)
1. Adapts for non-narrative content (context → main points → insights → applications)
1. Writes in first-person as if I’m describing visual moments from my own perspective
1. Produces concrete descriptions rather than abstract conceptual summaries

**Example output difference:**

- Third-person analyst mode: “The author discusses the problem of prompt drift”
- First-person reflection mode: “I showed how prompts lose effectiveness by comparing a fresh prompt to one used repeatedly”

**Why Gemini 2.5 Flash:** Fast processing for straightforward extraction tasks, and the Gems feature (which is Google’s version of system prompts) makes it easy to set persistent instructions without managing project contexts.

**Why this works:** First-person framing naturally emphasizes tangible, visualizable elements instead of abstract concepts. I’ve found this dramatically improves image generation quality.

---

### **Step 6: Image Generation with IMG Generator**

*Time: 15-25 minutes (including selection and refinement)*

**Tool: IMG Generator**

- Platform: Microsoft Copilot iOS app (quick response mode with DALL-E)
- Input: First-person narrative beats from Narrative Analyzer
- Output: 1920x1920px images

I take the narrative beats and feed them into my image generation assistant, which creates featured images and section visuals.

**How it works:**\
The assistant uses a **modular template system**:

**Base Templates:**

- **Title**: Main article image with typography
- **Section**: Visual breaks without text

**Modifiers (combinable):**

- **-Metaphor**: Symbolic/abstract imagery instead of literal
- **-Invert**: Swaps color palette
- **-Collage**: Textured icon background

**Brand Consistency Elements:**

- Locked color palette (teal, purple, yellow, coral, blue, pink)
- Character design (matches my appearance: brown skin, dark hair, glasses)
- Geometric composition, flat vector style
- 1920x1920px dimensions

**Example workflow:**

1. Input: “Title-Metaphor-Collage: [narrative beat about prompt engineering]”
1. Assistant generates list of 10-12 relevant icons (gear, lightbulb, chat bubble, code brackets, etc.)
1. Constructs DALL-E prompt combining template + modifiers + icons + brand elements
1. Generates image
1. Provides 150-character explanation of translation decisions

**Why Microsoft Copilot:** Direct DALL-E integration with quick response mode for rapid iteration, and the iOS app makes it easy to generate images on the go.

**Why this approach works:** The modifier system maintains structural consistency while allowing conceptual variation. `Title-Metaphor-Collage` keeps the title layout but shifts to symbolic imagery with textured backgrounds. This prevents style drift across posts while adapting to different content needs. The required icon generation step for collages prevents generic outputs. Forcing the assistant to list specific icons before building the prompt produces more thoughtful, concept-relevant backgrounds.

---

### **Final Pass: Quality Check (Human)**

*Time: 10-15 minutes*

I do a final read-through of the complete package:

- Post reads smoothly and delivers value
- Images enhance rather than distract
- All claims are accurate and defensible
- Technical details are correct
- The post solves a real problem or teaches something useful

Only then do I publish.

---

## What Makes This Multi-Platform Approach Work

You might wonder why I use three different platforms (Claude, Gemini, Copilot) instead of doing everything in one place. Here’s my reasoning:

**Claude Sonnet 4.5 for complex reasoning tasks:** BlogBuilder needs strong reasoning to analyze writing patterns and provide nuanced feedback. The LLM Prompt Engineer requires sophisticated prompt architecture capabilities. Long context window essential for working with multiple writing examples.

**Gemini 2.5 Flash for fast extraction:** Narrative Analyzer is a straightforward extraction task that doesn’t need heavy reasoning. Gems feature makes persistent system prompts incredibly convenient. Speed matters when I just need structured output.

**Microsoft Copilot for visual generation:** Direct DALL-E integration without API complexity. Quick response mode perfect for iterative image generation. iOS app accessibility when working mobile.

Each platform excels at specific tasks. Using the right tool for each job produces better results than forcing everything through a single interface.

---

## Key Insights from Building This Workflow

**Specialized beats general-purpose:** A single “do everything” assistant produces mediocre results across all tasks. Specialized assistants with focused capabilities consistently outperform generalists.

**Fresh context prevents bias:** Starting a new chat with BlogBuilder for review mode is essential. The assistant needs to evaluate my edited work objectively, not defend its original output.

**First-person beats third-person for image prompts:** Having the Narrative Analyzer write as me rather than about me produces dramatically better visual descriptions.

**Modular systems scale:** The IMG Generator’s template + modifier system lets me maintain brand consistency while adapting to new content needs without redesigning the entire prompt.

**Phase gates prevent premature optimization:** The LLM Prompt Engineer’s structured approach saves time by ensuring I fully understand requirements before building solutions.

---

## The Meta Reality

Yes, this page about my AI process was drafted using BlogBuilder, edited heavily by me, and refined through multiple passes. The workflow described above is the same workflow that created this documentation.

That’s the point: AI is a tool that amplifies human expertise when used deliberately and thoughtfully. The quality of the output depends entirely on:

1. The quality of the input (my expertise and insights)
1. The quality of the prompt engineering (my assistant design)
1. The quality of the editing (my editorial judgment)
1. The appropriateness of the tool selection (matching platforms to tasks)

---

## Questions?

I’m always happy to discuss my process, assistant design philosophy, or prompt engineering approach in more detail. Send an email to [contact@uxpromptlab.com](mailto:contact@uxpromptlab.com).

*Last updated: 2025.11.08*
