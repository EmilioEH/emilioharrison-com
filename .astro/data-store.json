[["Map",1,2,7,8],"meta::meta",["Map",3,4,5,6],"astro-version","5.16.1","astro-config-digest","{\"root\":{},\"srcDir\":{},\"publicDir\":{},\"outDir\":{},\"cacheDir\":{},\"compressHTML\":true,\"base\":\"/\",\"trailingSlash\":\"ignore\",\"output\":\"static\",\"scopedStyleStrategy\":\"attribute\",\"build\":{\"format\":\"directory\",\"client\":{},\"server\":{},\"assets\":\"_astro\",\"serverEntry\":\"entry.mjs\",\"redirects\":true,\"inlineStylesheets\":\"auto\",\"concurrency\":1},\"server\":{\"open\":false,\"host\":false,\"port\":4321,\"streaming\":true,\"allowedHosts\":[]},\"redirects\":{},\"image\":{\"endpoint\":{\"route\":\"/_image\"},\"service\":{\"entrypoint\":\"astro/assets/services/sharp\",\"config\":{}},\"domains\":[],\"remotePatterns\":[],\"responsiveStyles\":false},\"devToolbar\":{\"enabled\":true},\"markdown\":{\"syntaxHighlight\":{\"type\":\"shiki\",\"excludeLangs\":[\"math\"]},\"shikiConfig\":{\"langs\":[],\"langAlias\":{},\"theme\":\"github-dark\",\"themes\":{},\"wrap\":false,\"transformers\":[]},\"remarkPlugins\":[],\"rehypePlugins\":[],\"remarkRehype\":{},\"gfm\":true,\"smartypants\":true},\"security\":{\"checkOrigin\":true,\"allowedDomains\":[]},\"env\":{\"schema\":{},\"validateSecrets\":false},\"experimental\":{\"clientPrerender\":false,\"contentIntellisense\":false,\"headingIdCompat\":false,\"preserveScriptOrder\":false,\"liveContentCollections\":false,\"csp\":false,\"staticImportMetaEnv\":false,\"chromeDevtoolsWorkspace\":false,\"failOnPrerenderConflict\":false,\"svgo\":false},\"legacy\":{\"collections\":false}}","posts",["Map",9,10,54,55,126,127,185,186,237,238],"blog-when-your-ai-breakthrough-doesnt-save-anyone-time-20251126",{"id":9,"data":11,"body":26,"filePath":27,"digest":28,"rendered":29,"legacyId":53},{"title":12,"date":13,"category":14,"excerpt":15,"takeaways":16},"“When Your AI Breakthrough Doesn’t Save Anyone Time”","“Nov 26, 2025”","“AI & UX”","“Learn why asking “what job needs doing?” before building with AI can save you from creating solutions nobody needs.​​​​​​​​​​​​​​​​”",[17,20,23],{"title":18,"text":19},"“Ask ‘What job is this solving?’ before building”","“Before jumping into development, validate that there’s actually a painful problem to solve. Ask what people currently use, why it’s insufficient, and whether a new solution is genuinely needed—not just technically possible.”",{"title":21,"text":22},"“Technical curiosity can blind you to user needs”","“Getting excited about whether you *can* build something with AI often means skipping the question of whether you *should*. That fascination with solving the technical puzzle needs to be balanced with understanding actual user pain points.”",{"title":24,"text":25},"“Match your approach to the stakes”","“Low-stakes personal experiments? Follow your curiosity. High-stakes team tools? Do proper discovery first. The challenge is recognizing which situation you’re in before you’ve already built the wrong thing.“​​​​​​​​​​​​​​​​","# When Your AI Breakthrough Doesn’t Save Anyone Time\n\n![*Title*](/images/blog/When Your AI Breakthrough Doesn’t Save Anyone Time-20251126/IMG_7229.jpeg)\n\n## What job was it being hired for?\n\nI closed MS Teams and logged off. Mid-afternoon on a Tuesday. Just shut it down and walked away from my desk.\n\nA few hours earlier, I demoed a Figma plugin I built for the team. Something to help with design system documentation work. The demo went well. People seemed excited. There was celebration. I felt good.\n\nThen I pinged my coworker to se how it was working and to see if it was saving any time compared to the old way. Nothing official, just a quick check in.\n\nHe responded, “oh ya it’s working. Not really saving much time because this is already a quick process. It’s actually pretty easy.”\n\nI messaged another teammate. Same thing. Then he sent me a list of other plugins that would ACTUALLY be time savers.\n\nThat’s when I closed Teams.\n\n-----\n\n![*Telescope*](/images/blog/When Your AI Breakthrough Doesn’t Save Anyone Time-20251126/IMG_7233.jpeg)\n\nA month later I realized the problem with the plugin was that I never asked “what job was my plugin being hired to do?”\n\nI focused purely on the technical questions like “what does it do” or “how does it work”, but never the Jobs to be Done question: what painful job were people hiring my plugin to solve?\n\nThe answer in this case?\n\nNothing. There was no painful job.\n\nThe documentation work was already quick. Already easy. They’d already hired something, their current process, and it worked fine.\n\nI’d built a solution to a problem that didn’t exist.\n\n-----\n\n![*Map vs Building*](/images/blog/When Your AI Breakthrough Doesn’t Save Anyone Time-20251126/IMG_7234.jpeg)\n\nOver that month, I was working on a different project that *was* using Jobs to be Done. More formal process: watching users, documenting observations, creating JTBD maps based on what we saw.\n\nThe contrast between the two approaches became impossible to ignore.\n\nThe JTBD work forced us to ask *what should we build?* before anything else. What job are people trying to get done? What are they currently hiring to do that job? Why is their solution insufficient?\n\nThe Figma plugin? I’d only asked *how do I build what I was asked for?*\n\nMy coworker said “can you build something to help with documentation” and I immediately started thinking: *how do I solve this technical problem? What’s the right approach? Can I actually pull this off?*\n\nI never asked if documentation was actually painful for them. What’s the current process? Why does it need fixing?\n\nThe difference between a real project and an ad hoc request. JTBD led to understanding what we should build. The plugin was just about figuring out how to build the thing I was told to build.\n\nYou can read more about the technical side—how I eventually figured out the “how”—in [Context is King](https://uxpromptlab.com/blogs/lab-notes/context-is-king). But that’s not really the point.\n\nThe point is I built something nobody needed because I never asked if they needed it.\n\n## What was I actually excited about?\n\n![*AI House of Cards*](/images/blog/When Your AI Breakthrough Doesn’t Save Anyone Time-20251126/IMG_7235.jpeg)\n\n*Why did I jump straight to building?*\n\nHonestly, I was excited about seeing if I could build something technical with AI.\n\nNot solving a problem anyone had. Solving the challenge of making it work.\n\nWhen my coworker asked about the plugin, I heard “Figma plugin for documentation” and my brain immediately went: *This is possible with AI. How do I make it work? I need to learn more.*\n\nThat curiosity is genuine. It’s not about impressing people or proving myself to others. It’s about being genuinely fascinated by what’s suddenly possible.\n\n*Wow, this is possible? How do I solve this technical problem?*\n\nI got so absorbed in that technical puzzle that I never stepped back to ask whether the thing should exist.\n\n“Your scientists were so preoccupied with whether or not they could, they didn’t stop to think if they should.” - Dr. Ian Malcolm, Jurassic Park\n\nNot quite as dramatic as creating terrorizing dinosaurs, but the principle still stands.\n\n## When is that excitement the right response?\n\n![*Solving the Puzzle*](/IMG_7236.jpeg)\n\nSometimes that absorption in the technical puzzle IS the right move.\n\nLearning. Experimenting. Exploring what’s possible beyond the hype. That matters. That’s how you figure out what AI can actually do.\n\nBut sometimes it means I build something nobody needs.\n\nSince I’ve had some time since this realization, I think I’ve come up with some criteria to consider when I’m in this situation:\n\n- **Stakes:** Low stakes, experiment freely. High stakes, slow down and ask questions first.\n- **Scope:** Quick one-off for yourself, just build it. Tool for the whole team, do discovery.\n- **Role:** Personal learning, follow the curiosity. Professional delivery, follow the process.\n\nThose all make sense when I write them out like this, but in the moment when I’m staring at an interesting technical problem?\n\nI’ll probably still just start building.\n\n## Where’s the line?\n\n![*Balancing Act*](/images/blog/When Your AI Breakthrough Doesn’t Save Anyone Time-20251126/IMG_7239.jpeg)\n\nI haven’t actually applied Jobs to be Done to an AI project yet.\n\nPartly because I haven’t had the right project. But mostly because the ad-hoc nature of AI experimentation is too exciting. I get wrapped up in figuring out what’s possible. The technical puzzle pulls me in before I remember to slow down.\n\nI’m trying to find the balance between that excitement and working diligently the right way.\n\nI know the formal JTBD process works. It led to understanding what we should build, but it’s hard in the moment to force that intentionality into the puzzle work of AI. \n\nNext time I get excited about building something with AI, I’m going to TRY asking “what job is this being hired for?” first. I’m going to try talking to the people who would use it. I’m going to try asking what’s actually painful about their current process.\n\nBut I’ll probably still get absorbed in the technical puzzle immediately. I’ll probably still jump to “how do I make this work?” before I’ve fully explored whether anyone needs it to work.\n\nOR\n\nOn the whole side, I might over-correct. I might spend too much time on discovery and kill the experimental momentum. I might lose what makes this work exciting.\n\nI’d rather fail by being too careful than build another plugin that gets a polite “oh ya it’s working” before everyone goes back to their old way of doing things.\n\nBut I don’t know if that’s actually true. I don’t know if I’ll slow down when I should. I don’t know where the line is between necessary experimentation and building things nobody needs.\n\n-----\n\n*If you’ve figured out how to balance AI experimentation with understanding actual user needs, I want to hear about it. Because I’m actively trying to find that balance, and I don’t have good answers yet.*","src/content/posts/Blog-When Your AI Breakthrough Doesn’t Save Anyone Time-20251126.md","b921399c4d108776",{"html":30,"metadata":31},"\u003Ch1 id=\"when-your-ai-breakthrough-doesnt-save-anyone-time\">When Your AI Breakthrough Doesn’t Save Anyone Time\u003C/h1>\n\u003Cp>![\u003Cem>Title\u003C/em>](/images/blog/When Your AI Breakthrough Doesn’t Save Anyone Time-20251126/IMG_7229.jpeg)\u003C/p>\n\u003Ch2 id=\"what-job-was-it-being-hired-for\">What job was it being hired for?\u003C/h2>\n\u003Cp>I closed MS Teams and logged off. Mid-afternoon on a Tuesday. Just shut it down and walked away from my desk.\u003C/p>\n\u003Cp>A few hours earlier, I demoed a Figma plugin I built for the team. Something to help with design system documentation work. The demo went well. People seemed excited. There was celebration. I felt good.\u003C/p>\n\u003Cp>Then I pinged my coworker to se how it was working and to see if it was saving any time compared to the old way. Nothing official, just a quick check in.\u003C/p>\n\u003Cp>He responded, “oh ya it’s working. Not really saving much time because this is already a quick process. It’s actually pretty easy.”\u003C/p>\n\u003Cp>I messaged another teammate. Same thing. Then he sent me a list of other plugins that would ACTUALLY be time savers.\u003C/p>\n\u003Cp>That’s when I closed Teams.\u003C/p>\n\u003Chr>\n\u003Cp>![\u003Cem>Telescope\u003C/em>](/images/blog/When Your AI Breakthrough Doesn’t Save Anyone Time-20251126/IMG_7233.jpeg)\u003C/p>\n\u003Cp>A month later I realized the problem with the plugin was that I never asked “what job was my plugin being hired to do?”\u003C/p>\n\u003Cp>I focused purely on the technical questions like “what does it do” or “how does it work”, but never the Jobs to be Done question: what painful job were people hiring my plugin to solve?\u003C/p>\n\u003Cp>The answer in this case?\u003C/p>\n\u003Cp>Nothing. There was no painful job.\u003C/p>\n\u003Cp>The documentation work was already quick. Already easy. They’d already hired something, their current process, and it worked fine.\u003C/p>\n\u003Cp>I’d built a solution to a problem that didn’t exist.\u003C/p>\n\u003Chr>\n\u003Cp>![\u003Cem>Map vs Building\u003C/em>](/images/blog/When Your AI Breakthrough Doesn’t Save Anyone Time-20251126/IMG_7234.jpeg)\u003C/p>\n\u003Cp>Over that month, I was working on a different project that \u003Cem>was\u003C/em> using Jobs to be Done. More formal process: watching users, documenting observations, creating JTBD maps based on what we saw.\u003C/p>\n\u003Cp>The contrast between the two approaches became impossible to ignore.\u003C/p>\n\u003Cp>The JTBD work forced us to ask \u003Cem>what should we build?\u003C/em> before anything else. What job are people trying to get done? What are they currently hiring to do that job? Why is their solution insufficient?\u003C/p>\n\u003Cp>The Figma plugin? I’d only asked \u003Cem>how do I build what I was asked for?\u003C/em>\u003C/p>\n\u003Cp>My coworker said “can you build something to help with documentation” and I immediately started thinking: \u003Cem>how do I solve this technical problem? What’s the right approach? Can I actually pull this off?\u003C/em>\u003C/p>\n\u003Cp>I never asked if documentation was actually painful for them. What’s the current process? Why does it need fixing?\u003C/p>\n\u003Cp>The difference between a real project and an ad hoc request. JTBD led to understanding what we should build. The plugin was just about figuring out how to build the thing I was told to build.\u003C/p>\n\u003Cp>You can read more about the technical side—how I eventually figured out the “how”—in \u003Ca href=\"https://uxpromptlab.com/blogs/lab-notes/context-is-king\">Context is King\u003C/a>. But that’s not really the point.\u003C/p>\n\u003Cp>The point is I built something nobody needed because I never asked if they needed it.\u003C/p>\n\u003Ch2 id=\"what-was-i-actually-excited-about\">What was I actually excited about?\u003C/h2>\n\u003Cp>![\u003Cem>AI House of Cards\u003C/em>](/images/blog/When Your AI Breakthrough Doesn’t Save Anyone Time-20251126/IMG_7235.jpeg)\u003C/p>\n\u003Cp>\u003Cem>Why did I jump straight to building?\u003C/em>\u003C/p>\n\u003Cp>Honestly, I was excited about seeing if I could build something technical with AI.\u003C/p>\n\u003Cp>Not solving a problem anyone had. Solving the challenge of making it work.\u003C/p>\n\u003Cp>When my coworker asked about the plugin, I heard “Figma plugin for documentation” and my brain immediately went: \u003Cem>This is possible with AI. How do I make it work? I need to learn more.\u003C/em>\u003C/p>\n\u003Cp>That curiosity is genuine. It’s not about impressing people or proving myself to others. It’s about being genuinely fascinated by what’s suddenly possible.\u003C/p>\n\u003Cp>\u003Cem>Wow, this is possible? How do I solve this technical problem?\u003C/em>\u003C/p>\n\u003Cp>I got so absorbed in that technical puzzle that I never stepped back to ask whether the thing should exist.\u003C/p>\n\u003Cp>“Your scientists were so preoccupied with whether or not they could, they didn’t stop to think if they should.” - Dr. Ian Malcolm, Jurassic Park\u003C/p>\n\u003Cp>Not quite as dramatic as creating terrorizing dinosaurs, but the principle still stands.\u003C/p>\n\u003Ch2 id=\"when-is-that-excitement-the-right-response\">When is that excitement the right response?\u003C/h2>\n\u003Cp>\u003Cimg src=\"/IMG_7236.jpeg\" alt=\"Solving the Puzzle\">\u003C/p>\n\u003Cp>Sometimes that absorption in the technical puzzle IS the right move.\u003C/p>\n\u003Cp>Learning. Experimenting. Exploring what’s possible beyond the hype. That matters. That’s how you figure out what AI can actually do.\u003C/p>\n\u003Cp>But sometimes it means I build something nobody needs.\u003C/p>\n\u003Cp>Since I’ve had some time since this realization, I think I’ve come up with some criteria to consider when I’m in this situation:\u003C/p>\n\u003Cul>\n\u003Cli>\u003Cstrong>Stakes:\u003C/strong> Low stakes, experiment freely. High stakes, slow down and ask questions first.\u003C/li>\n\u003Cli>\u003Cstrong>Scope:\u003C/strong> Quick one-off for yourself, just build it. Tool for the whole team, do discovery.\u003C/li>\n\u003Cli>\u003Cstrong>Role:\u003C/strong> Personal learning, follow the curiosity. Professional delivery, follow the process.\u003C/li>\n\u003C/ul>\n\u003Cp>Those all make sense when I write them out like this, but in the moment when I’m staring at an interesting technical problem?\u003C/p>\n\u003Cp>I’ll probably still just start building.\u003C/p>\n\u003Ch2 id=\"wheres-the-line\">Where’s the line?\u003C/h2>\n\u003Cp>![\u003Cem>Balancing Act\u003C/em>](/images/blog/When Your AI Breakthrough Doesn’t Save Anyone Time-20251126/IMG_7239.jpeg)\u003C/p>\n\u003Cp>I haven’t actually applied Jobs to be Done to an AI project yet.\u003C/p>\n\u003Cp>Partly because I haven’t had the right project. But mostly because the ad-hoc nature of AI experimentation is too exciting. I get wrapped up in figuring out what’s possible. The technical puzzle pulls me in before I remember to slow down.\u003C/p>\n\u003Cp>I’m trying to find the balance between that excitement and working diligently the right way.\u003C/p>\n\u003Cp>I know the formal JTBD process works. It led to understanding what we should build, but it’s hard in the moment to force that intentionality into the puzzle work of AI.\u003C/p>\n\u003Cp>Next time I get excited about building something with AI, I’m going to TRY asking “what job is this being hired for?” first. I’m going to try talking to the people who would use it. I’m going to try asking what’s actually painful about their current process.\u003C/p>\n\u003Cp>But I’ll probably still get absorbed in the technical puzzle immediately. I’ll probably still jump to “how do I make this work?” before I’ve fully explored whether anyone needs it to work.\u003C/p>\n\u003Cp>OR\u003C/p>\n\u003Cp>On the whole side, I might over-correct. I might spend too much time on discovery and kill the experimental momentum. I might lose what makes this work exciting.\u003C/p>\n\u003Cp>I’d rather fail by being too careful than build another plugin that gets a polite “oh ya it’s working” before everyone goes back to their old way of doing things.\u003C/p>\n\u003Cp>But I don’t know if that’s actually true. I don’t know if I’ll slow down when I should. I don’t know where the line is between necessary experimentation and building things nobody needs.\u003C/p>\n\u003Chr>\n\u003Cp>\u003Cem>If you’ve figured out how to balance AI experimentation with understanding actual user needs, I want to hear about it. Because I’m actively trying to find that balance, and I don’t have good answers yet.\u003C/em>\u003C/p>",{"headings":32,"localImagePaths":50,"remoteImagePaths":51,"frontmatter":11,"imagePaths":52},[33,37,41,44,47],{"depth":34,"slug":35,"text":36},1,"when-your-ai-breakthrough-doesnt-save-anyone-time","When Your AI Breakthrough Doesn’t Save Anyone Time",{"depth":38,"slug":39,"text":40},2,"what-job-was-it-being-hired-for","What job was it being hired for?",{"depth":38,"slug":42,"text":43},"what-was-i-actually-excited-about","What was I actually excited about?",{"depth":38,"slug":45,"text":46},"when-is-that-excitement-the-right-response","When is that excitement the right response?",{"depth":38,"slug":48,"text":49},"wheres-the-line","Where’s the line?",[],[],[],"Blog-When Your AI Breakthrough Doesn’t Save Anyone Time-20251126.md","context-is-king",{"id":54,"data":56,"body":83,"filePath":84,"digest":85,"rendered":86,"legacyId":125},{"title":57,"date":58,"category":59,"excerpt":60,"takeaways":61},"Context is King","Nov 26, 2025","AI & UX","Learn why 'context engineering,' feeding LLMs documentation, outperforms complex prompting. Turn one-off solutions into scalable tools.",[62,65,68,71,74,77,80],{"title":63,"text":64},"Context Over Prompts","Stop trying to perfect your prompt wording. Instead, provide AI with the right documentation and information. It's like sending someone a map instead of trying to describe directions perfectly.",{"title":66,"text":67},"Context Engineering vs Prompt Engineering","Building with AI is less about finding perfect phrases and more about determining what context (documentation, examples, specifications) will generate the desired behavior.",{"title":69,"text":70},"The Documentation Solution","The author solved a Figma plugin challenge by feeding Claude the official Figma API documentation, not by refining prompts. The AI generated working code by having the right reference material.",{"title":72,"text":73},"Build the Builder","Create reusable templates with documentation that others can use. One solution becomes scalable when you teach others how to inject the right context for their own problems.",{"title":75,"text":76},"Human-Driven RAG","Your role is to be a curator of information, not a prompt expert. Find the right source material (official docs, guides, specifications) and provide it to the AI.",{"title":78,"text":79},"Limitations","Context engineering only works when good source material exists. It excels at technical integration and established systems with documentation, but won't solve problems requiring subjective creative judgment.",{"title":81,"text":82},"Practical Approach","Open two tabs: one with official documentation for your system, one with your LLM. Copy relevant sections into the LLM, describe your problem in plain language, and see what it builds.","# Context is King\n\nThe MS Teams notification lit up my screen at 3:30 PM.\n\nI was deep in the work of building a survey for a project that wasn't interesting at all, but important stakeholders were waiting on it, so I had to deliver. I'd been clicking screener questions and likert rating scales for the past hour.\n\nThe message preview: Hey, you around for a quick call? Need your AI brain.\n\nHonestly? I was relieved. An excuse to do something actually interesting.\n\nMy coworker was stuck documenting a complex UI component sheet. Tedious, mind-numbing work. Could I \"vibe code something\" with AI to fix it?\n\nI hesitated before responding.\n\nNot because of the task. Because I knew my limits. I'm great at the quick wins, the standalone AI hacks. But deep system integration? Building something that integrates with Figma? I had doubts.\n\nI told him I'd look into it. I wasn't sure I actually could.\n\n## The Fear Under the Block\n\nWhat I didn't say on that call was that I didn't want to admit I might not be able to help.\n\nA few months earlier, I'd built an LLM-powered heuristic evaluation assistant and a thematic analysis assistant. Both got rolled out company-wide to all UX team members. They were getting attention. A month before this Figma request, I'd led my team to second place in a company-wide AI ideathon.\n\nI'd become the defacto AI guy. The person who teaches people about LLMs. The one who uses AI in creative ways to solve difficult problems.\n\nAnd now I was stuck on a Figma plugin, the exact kind of creative problem I was supposed to excel at.\n\nI'd been in this exact spot before. I sold vibe coding as something that would lead to \"UX Engineering.\" Because of that I was put on a project to vibe code a design system documentation site. Trying different prompts, tweaking the wording, hoping the right combination of words would unlock the solution. It never did.\n\nPretty soon I was in over my head and we had to bring in real developers. It was humbling.\n\nThe truth is that I was treating AI like a magic spell. Find the perfect incantation, get the perfect result.\n\nThat's exhausting and it isn't a scalable approach.\n\n## The Moment Everything Shifted\n\nLet me take you back to that full day…\n\nEarlier that morning, after standup, a coworker pulled me into an impromptu Zoom. They wanted to learn how to use LLMs better. I taught them the basics of prompting—you know, role assignment, temperature settings, that kind of thing. Then I walked them through context injection—the idea that you can feed an LLM specific information to guide its responses. We spent thirty minutes on it. I love this part of the work. This coworker was a self-described \"Luddite.\" Seeing them flip to actively seeking AI guidance? I was thrilled.\n\nI wrapped the call feeling good. Like I'd actually helped someone understand something useful.\n\nThen later that afternoon, my other coworker pinged me about the Figma plugin.\n\nI opened Claude. Tried different prompts. Refined the wording. Spent thirty minutes going in circles, the frustration building.\n\nThen something clicked.\n\nWait.\n\nThat conversation this morning. Context injection. What if the Figma problem wasn't a coding problem at all? What if it was just… a context problem?\n\nI was so convinced this would work that I raced to find the right context. But the whole time I couldn't stop thinking…\n\nIt can't be this easy.\n\nThe realization is that you don't need to memorize the recipe. You just need to know where the cookbook is.\n\n## What Context Engineering Actually Means\n\nI kept hearing \"prompt engineering\" everywhere. Optimize your prompts. Find the perfect phrasing. Make the AI do what you want.\n\nBut that's backwards.\n\nAnthropic puts it perfectly: \"Building with language models is becoming less about finding the right words and phrases for your prompts, and more about answering the broader question of 'what configuration of context is most likely to generate our model's desired behavior?'\"\n\nThink about giving someone directions. You can spend ten minutes trying to describe the route perfectly: \"Turn left at the third light, no wait, the fourth light, or is it after the gas station?\"\n\nOr you can just send them a map.\n\nThe map is the context. Stop perfecting your description. Send the map.\n\n## The Figma Documentation Experiment\n\nI pulled up the official Figma Academy developer documentation. All of it. I didn't read through it carefully. I didn't need to understand it myself. I just needed to give it to the AI.\n\nI copied the relevant sections: the API structure, the plugin examples, the method calls. Then I went back to Claude.\n\nThis time, I wasn't asking for a quick fix. I was setting up an environment where the AI could actually understand what I needed. I described the user experience we wanted: a tool that would automatically generate documentation for UI components.\n\nI hit send and watched it generate code.\n\nReal code. Code that actually referenced the correct Figma API methods. Code that I definitely couldn't have written myself.\n\nI sat there staring at my screen thinking: \"There's no way this actually works.\"\n\n### Testing the Impossible\n\nI copied the code into the Figma plugin editor. I was typing fast, trying to move quickly before I lost the momentum of the idea. I wanted to know if this would actually work or if I was just fooling myself.\n\nThis felt too easy. Years of required dev experience, bypassed by copying documentation?\n\nI hit run.\n\nThe plugin loaded. No errors.\n\nI opened a component in Figma, ran the plugin, and watched it pull the component properties and generate formatted documentation. Automatically. Exactly what my coworker needed.\n\nI messaged him: Got something. Want to see it?\n\nHis response: Wait, seriously? Already?\n\n### The Parking Lot Demo\n\nThe next morning, after standup, we went into our parking lot. The 15-minute open slot for quick demos and questions.\n\nI shared my screen and ran the plugin in Figma. The documentation generated in seconds.\n\n\"Okay, that's great,\" my coworker said. \"Can you make me one for layout specs too?\"\n\nThat's when I showed them the actual breakthrough.\n\nI pulled up the documentation I'd saved in Jira. A how-to file with the Figma docs and a template prompt. \"This is how I built it. Anyone can use this to build their own plugins.\"\n\nMy coworker opened Claude on his second monitor, copied the template, and started describing the layout tool he needed. We all watched the code generate in real-time.\n\nHe looked at the screen. \"Wait, it's really that simple?\"\n\nYou could see it click for everyone. They got it.\n\n## Building the Builder\n\nI didn't just solve one problem. I built a tool for them to solve their own problems. It's a bit meta, but this type of thinking is what turns a one off solution to a scalable approach for the whole team.\n\nOnce you inject the right context (the specific documentation, the precise API structure), the AI stops being a single-task assistant. It becomes a tool that builds other tools.\n\nMy coworker didn't need me anymore. He needed the documentation and basic knowledge of how to frame the request. That's it.\n\nWhich was the goal, obviously. But I couldn't stop thinking about that moment.\n\nI guess some part of me wondered: how many more times would I create AI solutions or teach myself out of work? Maybe that's dramatic. But it does feel like what all the headlines and AI hype are leading to.\n\nThe thing is, I'm not sure that's actually bad. Just… different than I expected.\n\nOne person's problem became a solution for the whole team. That's the difference between clever prompting and actual context engineering. But, I'm digressing, this is probably a topic I'll explore in a different post.\n\n## Why Context Always Wins\n\nYou can spend hours perfecting your prompt. You can try different phrasings, different structures, different approaches.\n\nBut if the model doesn't have the right information, you're just hoping it guesses correctly.\n\nThe model has limited attention. Like your working memory when you're trying to remember a phone number while someone's talking to you. Why waste that attention making the AI figure out API structure when you can just provide it?\n\nThe skill isn't writing better prompts. It's knowing what information the AI is missing and where to find it.\n\nThis matters because it changes what skills you need to work effectively with AI.\n\nYou don't have to become an expert in prompt engineering, which is basically a whole field of study at this point, complete with frameworks and best practices and probably certification programs soon.\n\nYou have to learn how to find answers to your problems. And make good decisions about what's really needed to solve them.\n\nIt's sort of like human-driven RAG. Human in the loop. You're not trying to make the AI smarter through better prompting. You're making yourself a better curator of the information the AI needs.\n\nThat's a completely different skill. And honestly, it feels more sustainable than trying to keep up with the latest prompt engineering techniques.\n\n## The Honest Limitation\n\nThis approach isn't magic. It only works when you have good source material.\n\nIf your context is vague, contradictory, or requires subjective creative judgment, this falls apart. I can't inject context to make AI write a novel only I could write or design something that needs my specific aesthetic judgment.\n\nBut for technical integration? For working with established systems that have documentation? For building tools that follow existing patterns? Context engineering changes everything.\n\n## What You Can Do\n\nYou don't need to become a technical expert. You need to become a curator of good information.\n\nNext time you're stuck on a hard problem, try this: Open two tabs. In one, find the official docs for whatever system you're working with—developer guide, how to article, or the book about this topic. In the other, open your LLM.\n\nCopy the sections that seem relevant. Yes, all of them. Then describe what you need in plain language. Don't optimize your prompt. Just explain the problem like you're talking to a coworker who knows the system.\n\nSee what happens.\n\nYou might be surprised how much you can build without being the expert. You just need to know where the experts already wrote down what they know.\n\nYou might be the one giving demos in parking lots next week.","src/content/posts/context-is-king.md","a0b5f78934ae2bfc",{"html":87,"metadata":88},"\u003Ch1 id=\"context-is-king\">Context is King\u003C/h1>\n\u003Cp>The MS Teams notification lit up my screen at 3:30 PM.\u003C/p>\n\u003Cp>I was deep in the work of building a survey for a project that wasn’t interesting at all, but important stakeholders were waiting on it, so I had to deliver. I’d been clicking screener questions and likert rating scales for the past hour.\u003C/p>\n\u003Cp>The message preview: Hey, you around for a quick call? Need your AI brain.\u003C/p>\n\u003Cp>Honestly? I was relieved. An excuse to do something actually interesting.\u003C/p>\n\u003Cp>My coworker was stuck documenting a complex UI component sheet. Tedious, mind-numbing work. Could I “vibe code something” with AI to fix it?\u003C/p>\n\u003Cp>I hesitated before responding.\u003C/p>\n\u003Cp>Not because of the task. Because I knew my limits. I’m great at the quick wins, the standalone AI hacks. But deep system integration? Building something that integrates with Figma? I had doubts.\u003C/p>\n\u003Cp>I told him I’d look into it. I wasn’t sure I actually could.\u003C/p>\n\u003Ch2 id=\"the-fear-under-the-block\">The Fear Under the Block\u003C/h2>\n\u003Cp>What I didn’t say on that call was that I didn’t want to admit I might not be able to help.\u003C/p>\n\u003Cp>A few months earlier, I’d built an LLM-powered heuristic evaluation assistant and a thematic analysis assistant. Both got rolled out company-wide to all UX team members. They were getting attention. A month before this Figma request, I’d led my team to second place in a company-wide AI ideathon.\u003C/p>\n\u003Cp>I’d become the defacto AI guy. The person who teaches people about LLMs. The one who uses AI in creative ways to solve difficult problems.\u003C/p>\n\u003Cp>And now I was stuck on a Figma plugin, the exact kind of creative problem I was supposed to excel at.\u003C/p>\n\u003Cp>I’d been in this exact spot before. I sold vibe coding as something that would lead to “UX Engineering.” Because of that I was put on a project to vibe code a design system documentation site. Trying different prompts, tweaking the wording, hoping the right combination of words would unlock the solution. It never did.\u003C/p>\n\u003Cp>Pretty soon I was in over my head and we had to bring in real developers. It was humbling.\u003C/p>\n\u003Cp>The truth is that I was treating AI like a magic spell. Find the perfect incantation, get the perfect result.\u003C/p>\n\u003Cp>That’s exhausting and it isn’t a scalable approach.\u003C/p>\n\u003Ch2 id=\"the-moment-everything-shifted\">The Moment Everything Shifted\u003C/h2>\n\u003Cp>Let me take you back to that full day…\u003C/p>\n\u003Cp>Earlier that morning, after standup, a coworker pulled me into an impromptu Zoom. They wanted to learn how to use LLMs better. I taught them the basics of prompting—you know, role assignment, temperature settings, that kind of thing. Then I walked them through context injection—the idea that you can feed an LLM specific information to guide its responses. We spent thirty minutes on it. I love this part of the work. This coworker was a self-described “Luddite.” Seeing them flip to actively seeking AI guidance? I was thrilled.\u003C/p>\n\u003Cp>I wrapped the call feeling good. Like I’d actually helped someone understand something useful.\u003C/p>\n\u003Cp>Then later that afternoon, my other coworker pinged me about the Figma plugin.\u003C/p>\n\u003Cp>I opened Claude. Tried different prompts. Refined the wording. Spent thirty minutes going in circles, the frustration building.\u003C/p>\n\u003Cp>Then something clicked.\u003C/p>\n\u003Cp>Wait.\u003C/p>\n\u003Cp>That conversation this morning. Context injection. What if the Figma problem wasn’t a coding problem at all? What if it was just… a context problem?\u003C/p>\n\u003Cp>I was so convinced this would work that I raced to find the right context. But the whole time I couldn’t stop thinking…\u003C/p>\n\u003Cp>It can’t be this easy.\u003C/p>\n\u003Cp>The realization is that you don’t need to memorize the recipe. You just need to know where the cookbook is.\u003C/p>\n\u003Ch2 id=\"what-context-engineering-actually-means\">What Context Engineering Actually Means\u003C/h2>\n\u003Cp>I kept hearing “prompt engineering” everywhere. Optimize your prompts. Find the perfect phrasing. Make the AI do what you want.\u003C/p>\n\u003Cp>But that’s backwards.\u003C/p>\n\u003Cp>Anthropic puts it perfectly: “Building with language models is becoming less about finding the right words and phrases for your prompts, and more about answering the broader question of ‘what configuration of context is most likely to generate our model’s desired behavior?’”\u003C/p>\n\u003Cp>Think about giving someone directions. You can spend ten minutes trying to describe the route perfectly: “Turn left at the third light, no wait, the fourth light, or is it after the gas station?”\u003C/p>\n\u003Cp>Or you can just send them a map.\u003C/p>\n\u003Cp>The map is the context. Stop perfecting your description. Send the map.\u003C/p>\n\u003Ch2 id=\"the-figma-documentation-experiment\">The Figma Documentation Experiment\u003C/h2>\n\u003Cp>I pulled up the official Figma Academy developer documentation. All of it. I didn’t read through it carefully. I didn’t need to understand it myself. I just needed to give it to the AI.\u003C/p>\n\u003Cp>I copied the relevant sections: the API structure, the plugin examples, the method calls. Then I went back to Claude.\u003C/p>\n\u003Cp>This time, I wasn’t asking for a quick fix. I was setting up an environment where the AI could actually understand what I needed. I described the user experience we wanted: a tool that would automatically generate documentation for UI components.\u003C/p>\n\u003Cp>I hit send and watched it generate code.\u003C/p>\n\u003Cp>Real code. Code that actually referenced the correct Figma API methods. Code that I definitely couldn’t have written myself.\u003C/p>\n\u003Cp>I sat there staring at my screen thinking: “There’s no way this actually works.”\u003C/p>\n\u003Ch3 id=\"testing-the-impossible\">Testing the Impossible\u003C/h3>\n\u003Cp>I copied the code into the Figma plugin editor. I was typing fast, trying to move quickly before I lost the momentum of the idea. I wanted to know if this would actually work or if I was just fooling myself.\u003C/p>\n\u003Cp>This felt too easy. Years of required dev experience, bypassed by copying documentation?\u003C/p>\n\u003Cp>I hit run.\u003C/p>\n\u003Cp>The plugin loaded. No errors.\u003C/p>\n\u003Cp>I opened a component in Figma, ran the plugin, and watched it pull the component properties and generate formatted documentation. Automatically. Exactly what my coworker needed.\u003C/p>\n\u003Cp>I messaged him: Got something. Want to see it?\u003C/p>\n\u003Cp>His response: Wait, seriously? Already?\u003C/p>\n\u003Ch3 id=\"the-parking-lot-demo\">The Parking Lot Demo\u003C/h3>\n\u003Cp>The next morning, after standup, we went into our parking lot. The 15-minute open slot for quick demos and questions.\u003C/p>\n\u003Cp>I shared my screen and ran the plugin in Figma. The documentation generated in seconds.\u003C/p>\n\u003Cp>“Okay, that’s great,” my coworker said. “Can you make me one for layout specs too?”\u003C/p>\n\u003Cp>That’s when I showed them the actual breakthrough.\u003C/p>\n\u003Cp>I pulled up the documentation I’d saved in Jira. A how-to file with the Figma docs and a template prompt. “This is how I built it. Anyone can use this to build their own plugins.”\u003C/p>\n\u003Cp>My coworker opened Claude on his second monitor, copied the template, and started describing the layout tool he needed. We all watched the code generate in real-time.\u003C/p>\n\u003Cp>He looked at the screen. “Wait, it’s really that simple?”\u003C/p>\n\u003Cp>You could see it click for everyone. They got it.\u003C/p>\n\u003Ch2 id=\"building-the-builder\">Building the Builder\u003C/h2>\n\u003Cp>I didn’t just solve one problem. I built a tool for them to solve their own problems. It’s a bit meta, but this type of thinking is what turns a one off solution to a scalable approach for the whole team.\u003C/p>\n\u003Cp>Once you inject the right context (the specific documentation, the precise API structure), the AI stops being a single-task assistant. It becomes a tool that builds other tools.\u003C/p>\n\u003Cp>My coworker didn’t need me anymore. He needed the documentation and basic knowledge of how to frame the request. That’s it.\u003C/p>\n\u003Cp>Which was the goal, obviously. But I couldn’t stop thinking about that moment.\u003C/p>\n\u003Cp>I guess some part of me wondered: how many more times would I create AI solutions or teach myself out of work? Maybe that’s dramatic. But it does feel like what all the headlines and AI hype are leading to.\u003C/p>\n\u003Cp>The thing is, I’m not sure that’s actually bad. Just… different than I expected.\u003C/p>\n\u003Cp>One person’s problem became a solution for the whole team. That’s the difference between clever prompting and actual context engineering. But, I’m digressing, this is probably a topic I’ll explore in a different post.\u003C/p>\n\u003Ch2 id=\"why-context-always-wins\">Why Context Always Wins\u003C/h2>\n\u003Cp>You can spend hours perfecting your prompt. You can try different phrasings, different structures, different approaches.\u003C/p>\n\u003Cp>But if the model doesn’t have the right information, you’re just hoping it guesses correctly.\u003C/p>\n\u003Cp>The model has limited attention. Like your working memory when you’re trying to remember a phone number while someone’s talking to you. Why waste that attention making the AI figure out API structure when you can just provide it?\u003C/p>\n\u003Cp>The skill isn’t writing better prompts. It’s knowing what information the AI is missing and where to find it.\u003C/p>\n\u003Cp>This matters because it changes what skills you need to work effectively with AI.\u003C/p>\n\u003Cp>You don’t have to become an expert in prompt engineering, which is basically a whole field of study at this point, complete with frameworks and best practices and probably certification programs soon.\u003C/p>\n\u003Cp>You have to learn how to find answers to your problems. And make good decisions about what’s really needed to solve them.\u003C/p>\n\u003Cp>It’s sort of like human-driven RAG. Human in the loop. You’re not trying to make the AI smarter through better prompting. You’re making yourself a better curator of the information the AI needs.\u003C/p>\n\u003Cp>That’s a completely different skill. And honestly, it feels more sustainable than trying to keep up with the latest prompt engineering techniques.\u003C/p>\n\u003Ch2 id=\"the-honest-limitation\">The Honest Limitation\u003C/h2>\n\u003Cp>This approach isn’t magic. It only works when you have good source material.\u003C/p>\n\u003Cp>If your context is vague, contradictory, or requires subjective creative judgment, this falls apart. I can’t inject context to make AI write a novel only I could write or design something that needs my specific aesthetic judgment.\u003C/p>\n\u003Cp>But for technical integration? For working with established systems that have documentation? For building tools that follow existing patterns? Context engineering changes everything.\u003C/p>\n\u003Ch2 id=\"what-you-can-do\">What You Can Do\u003C/h2>\n\u003Cp>You don’t need to become a technical expert. You need to become a curator of good information.\u003C/p>\n\u003Cp>Next time you’re stuck on a hard problem, try this: Open two tabs. In one, find the official docs for whatever system you’re working with—developer guide, how to article, or the book about this topic. In the other, open your LLM.\u003C/p>\n\u003Cp>Copy the sections that seem relevant. Yes, all of them. Then describe what you need in plain language. Don’t optimize your prompt. Just explain the problem like you’re talking to a coworker who knows the system.\u003C/p>\n\u003Cp>See what happens.\u003C/p>\n\u003Cp>You might be surprised how much you can build without being the expert. You just need to know where the experts already wrote down what they know.\u003C/p>\n\u003Cp>You might be the one giving demos in parking lots next week.\u003C/p>",{"headings":89,"localImagePaths":122,"remoteImagePaths":123,"frontmatter":56,"imagePaths":124},[90,91,94,97,100,103,107,110,113,116,119],{"depth":34,"slug":54,"text":57},{"depth":38,"slug":92,"text":93},"the-fear-under-the-block","The Fear Under the Block",{"depth":38,"slug":95,"text":96},"the-moment-everything-shifted","The Moment Everything Shifted",{"depth":38,"slug":98,"text":99},"what-context-engineering-actually-means","What Context Engineering Actually Means",{"depth":38,"slug":101,"text":102},"the-figma-documentation-experiment","The Figma Documentation Experiment",{"depth":104,"slug":105,"text":106},3,"testing-the-impossible","Testing the Impossible",{"depth":104,"slug":108,"text":109},"the-parking-lot-demo","The Parking Lot Demo",{"depth":38,"slug":111,"text":112},"building-the-builder","Building the Builder",{"depth":38,"slug":114,"text":115},"why-context-always-wins","Why Context Always Wins",{"depth":38,"slug":117,"text":118},"the-honest-limitation","The Honest Limitation",{"depth":38,"slug":120,"text":121},"what-you-can-do","What You Can Do",[],[],[],"context-is-king.md","the-ai-expert-who-doesnt-know-what-a-token-is",{"id":126,"data":128,"body":153,"filePath":154,"digest":155,"rendered":156,"legacyId":184},{"title":129,"date":58,"category":59,"excerpt":130,"takeaways":131},"The AI Expert in the Room (Who Doesn't Know What a Token Is)","We're all faking expertise in AI. Learn why admitting 'I don't know' is more valuable than performing confidence you don't have.",[132,135,138,141,144,147,150],{"title":133,"text":134},"The Performative Expert Phenomenon","Many professionals are pretending to be AI experts while lacking fundamental understanding of concepts like tokens, context windows, and the difference between training and inference. This is happening across workplaces in meetings, Slack channels, and casual conversations.",{"title":136,"text":137},"Fear, Not Arrogance","The posturing comes from terror about job security and obsolescence. AI feels like an extinction-level event for knowledge work, and the constant messaging that you must be an expert or become irrelevant drives people to fake expertise as a survival strategy.",{"title":139,"text":140},"The Author's Own Struggle","As a UX Research expert, the author experienced their own mastery feeling 'quaint' when AI emerged. They caught themselves dropping buzzwords, deflecting follow-up questions, and performing confidence they didn't have rather than admitting gaps in knowledge.",{"title":142,"text":143},"The Brené Brown Insight","A quote shifted everything: 'The big shift here is from wanting to be right to wanting to get it right.' The focus on appearing valuable was preventing actual learning and keeping conversations shallow instead of deep.",{"title":145,"text":146},"The Cost of Performance","Pretending to know prevents real learning, keeps you in superficial conversations, and wastes energy managing an image instead of building genuine capability. The performance protects ego but blocks growth.",{"title":148,"text":149},"The Solution: Public Vulnerability","The author started saying 'I don't know' out loud in meetings and conversations. The result wasn't judgment but relief—others felt permission to also admit confusion, leading to collaborative learning and deeper understanding.",{"title":151,"text":152},"The Invitation","Notice the gap between what you're saying and what you actually understand. When you catch yourself or others performing expertise, offer an out: 'I'm still learning this too, want to figure it out together?' Honesty about where you are beats pretending to be the expert.","# The AI Expert in the Room (Who Doesn't Know What a Token Is)\n\n\"Oh yeah, I've been working with LLMs extensively,\" he says in the Zoom call, leaning back with that confident tech-bro energy. \"The key is just making sure your neural networks are aligned with your prompt architecture.\"\n\nI'm nodding along, but something feels off. Two minutes later, he's talking about \"training ChatGPT on our company data\" and how \"the AI learns from every conversation.\"\n\nI ask, \"Quick clarification, when you say training, do you mean fine-tuning or just adding context to prompts?\"\n\nSilence. Then, \"Yeah, exactly, training.\"\n\nHe doesn't know. And suddenly, I realize he has no idea what a token is, what a context window does, or the difference between inference and training. But he needed to sound like an expert. And I get it. I really do.\n\n## We're All Faking It\n\nThere's this thing happening right now in every Slack channel, every standup, every coffee chat about AI. People are posturing as experts while getting foundational concepts completely wrong.\n\nThe thing is, I don't think it's arrogance. I think it's terror.\n\nAI feels like an extinction-level event for knowledge work. Every article screams that our jobs are at risk. Every LinkedIn post shows someone claiming they \"10x'd their productivity with AI.\" The implicit message: If you're not an AI expert, you're obsolete.\n\nSo we perform expertise. We throw around \"large language models\" and \"prompt engineering\" and \"AI agents\" like shields against irrelevance.\n\n## I did it too\n\nFor the past 5 years, I've been an expert in UX Research. I knew the methodologies. I could moderate a usability test in my sleep. I had mastery, and that felt good. Safe.\n\nThen AI exploded, and suddenly I'm in conversations where people are talking about embeddings and vector databases and RAG architectures. My expertise felt… quaint. Like showing up to a rocket launch with a really nice bicycle.\n\nSo I did exactly what that guy in the Zoom call did. I started nodding along. Dropping terms I'd read in articles. Positioning myself as someone who \"gets it.\"\n\nIn one meeting, I confidently explained how we could \"leverage AI to automate our research synthesis\" without actually understanding what that would require.\n\nA designer asked a follow-up question about how the model would handle qualitative data, and I… deflected. Changed the subject. Performed confidence I didn't have.\n\nI was so afraid of looking like I was falling behind that I'd rather bullshit than admit I didn't know.\n\n## The Quote That Changed My POV\n\nThen I read this line from Brené Brown:\n\n\"The big shift here is from wanting to 'be right' to wanting to 'get it right.'\"\n\nIt hit me like a door to the face. I was so focused on appearing valuable, on being right about AI, that I'd stopped trying to actually learn. To get it right.\n\nThe posturing wasn't helping me. It was keeping me shallow. I was rehearsing answers instead of asking questions. I was protecting my ego instead of learning.\n\n## Why This Is So Hard\n\nHere's the thing I want to acknowledge: The fear is real.\n\nAI is changing knowledge work. Jobs are being redefined. The ground is shifting under all of us.\n\nWhen you've spent years building expertise in one domain, and suddenly there's a new domain that feels existential, of course you grasp for credibility. Of course you perform confidence you don't have.\n\nIt's not stupidity. It's survival instinct.\n\nAnd learning publicly, admitting you don't know something in front of colleagues, in front of your boss, in front of people who might be deciding your future, is genuinely vulnerable. It feels like handing someone ammunition.\n\nSometimes, in some rooms, a little strategic positioning is necessary. I get it.\n\nBut here's what I've learned: The performance has a cost. It keeps you from actually learning the thing you're pretending to know. It keeps you in shallow conversations instead of deep ones. And it makes you spend energy managing an image instead of building real capability.\n\n## What I'm Trying Now\n\nI started saying \"I don't know\" out loud.\n\nIn meetings. In Slack. In conversations with people I want to impress.\n\n\"I don't actually understand how fine-tuning works, can you explain that?\"\n\n\"I've read about RAG but I haven't implemented it. What's your experience been?\"\n\n\"I'm still figuring this out. Can we think through it together?\"\n\nAnd here's what happened: Nothing bad. No one thought I was incompetent. Most people seemed relieved to have permission to also not know everything.\n\nThe designer who asked me that follow-up question? We ended up having an hour-long conversation where we both admitted we were confused about the same things. We learned together. We actually got it right instead of pretending we already were.\n\n## An Invitation\n\nNext time you're in a conversation about AI or any new, scary thing try noticing the gap between what you're saying and what you actually understand.\n\nNotice if you're reaching for jargon to sound credible. Notice if you're nodding along when you're actually lost. Notice if someone else is doing the performance, and maybe offer them an out: \"I'm still learning this stuff too, want to figure it out together?\"\n\nThe shift from \"be right\" to \"get it right\" isn't easy. It won't feel safe at first.\n\nBut I'm finding it's the only way to actually learn. And maybe, the only way to build real expertise in something that's changing this fast.\n\nYou don't have to be the expert. You just have to be honest about where you are.","src/content/posts/the-ai-expert-who-doesnt-know-what-a-token-is.md","7cd4c7928069a397",{"html":157,"metadata":158},"\u003Ch1 id=\"the-ai-expert-in-the-room-who-doesnt-know-what-a-token-is\">The AI Expert in the Room (Who Doesn’t Know What a Token Is)\u003C/h1>\n\u003Cp>“Oh yeah, I’ve been working with LLMs extensively,” he says in the Zoom call, leaning back with that confident tech-bro energy. “The key is just making sure your neural networks are aligned with your prompt architecture.”\u003C/p>\n\u003Cp>I’m nodding along, but something feels off. Two minutes later, he’s talking about “training ChatGPT on our company data” and how “the AI learns from every conversation.”\u003C/p>\n\u003Cp>I ask, “Quick clarification, when you say training, do you mean fine-tuning or just adding context to prompts?”\u003C/p>\n\u003Cp>Silence. Then, “Yeah, exactly, training.”\u003C/p>\n\u003Cp>He doesn’t know. And suddenly, I realize he has no idea what a token is, what a context window does, or the difference between inference and training. But he needed to sound like an expert. And I get it. I really do.\u003C/p>\n\u003Ch2 id=\"were-all-faking-it\">We’re All Faking It\u003C/h2>\n\u003Cp>There’s this thing happening right now in every Slack channel, every standup, every coffee chat about AI. People are posturing as experts while getting foundational concepts completely wrong.\u003C/p>\n\u003Cp>The thing is, I don’t think it’s arrogance. I think it’s terror.\u003C/p>\n\u003Cp>AI feels like an extinction-level event for knowledge work. Every article screams that our jobs are at risk. Every LinkedIn post shows someone claiming they “10x’d their productivity with AI.” The implicit message: If you’re not an AI expert, you’re obsolete.\u003C/p>\n\u003Cp>So we perform expertise. We throw around “large language models” and “prompt engineering” and “AI agents” like shields against irrelevance.\u003C/p>\n\u003Ch2 id=\"i-did-it-too\">I did it too\u003C/h2>\n\u003Cp>For the past 5 years, I’ve been an expert in UX Research. I knew the methodologies. I could moderate a usability test in my sleep. I had mastery, and that felt good. Safe.\u003C/p>\n\u003Cp>Then AI exploded, and suddenly I’m in conversations where people are talking about embeddings and vector databases and RAG architectures. My expertise felt… quaint. Like showing up to a rocket launch with a really nice bicycle.\u003C/p>\n\u003Cp>So I did exactly what that guy in the Zoom call did. I started nodding along. Dropping terms I’d read in articles. Positioning myself as someone who “gets it.”\u003C/p>\n\u003Cp>In one meeting, I confidently explained how we could “leverage AI to automate our research synthesis” without actually understanding what that would require.\u003C/p>\n\u003Cp>A designer asked a follow-up question about how the model would handle qualitative data, and I… deflected. Changed the subject. Performed confidence I didn’t have.\u003C/p>\n\u003Cp>I was so afraid of looking like I was falling behind that I’d rather bullshit than admit I didn’t know.\u003C/p>\n\u003Ch2 id=\"the-quote-that-changed-my-pov\">The Quote That Changed My POV\u003C/h2>\n\u003Cp>Then I read this line from Brené Brown:\u003C/p>\n\u003Cp>“The big shift here is from wanting to ‘be right’ to wanting to ‘get it right.’”\u003C/p>\n\u003Cp>It hit me like a door to the face. I was so focused on appearing valuable, on being right about AI, that I’d stopped trying to actually learn. To get it right.\u003C/p>\n\u003Cp>The posturing wasn’t helping me. It was keeping me shallow. I was rehearsing answers instead of asking questions. I was protecting my ego instead of learning.\u003C/p>\n\u003Ch2 id=\"why-this-is-so-hard\">Why This Is So Hard\u003C/h2>\n\u003Cp>Here’s the thing I want to acknowledge: The fear is real.\u003C/p>\n\u003Cp>AI is changing knowledge work. Jobs are being redefined. The ground is shifting under all of us.\u003C/p>\n\u003Cp>When you’ve spent years building expertise in one domain, and suddenly there’s a new domain that feels existential, of course you grasp for credibility. Of course you perform confidence you don’t have.\u003C/p>\n\u003Cp>It’s not stupidity. It’s survival instinct.\u003C/p>\n\u003Cp>And learning publicly, admitting you don’t know something in front of colleagues, in front of your boss, in front of people who might be deciding your future, is genuinely vulnerable. It feels like handing someone ammunition.\u003C/p>\n\u003Cp>Sometimes, in some rooms, a little strategic positioning is necessary. I get it.\u003C/p>\n\u003Cp>But here’s what I’ve learned: The performance has a cost. It keeps you from actually learning the thing you’re pretending to know. It keeps you in shallow conversations instead of deep ones. And it makes you spend energy managing an image instead of building real capability.\u003C/p>\n\u003Ch2 id=\"what-im-trying-now\">What I’m Trying Now\u003C/h2>\n\u003Cp>I started saying “I don’t know” out loud.\u003C/p>\n\u003Cp>In meetings. In Slack. In conversations with people I want to impress.\u003C/p>\n\u003Cp>“I don’t actually understand how fine-tuning works, can you explain that?”\u003C/p>\n\u003Cp>“I’ve read about RAG but I haven’t implemented it. What’s your experience been?”\u003C/p>\n\u003Cp>“I’m still figuring this out. Can we think through it together?”\u003C/p>\n\u003Cp>And here’s what happened: Nothing bad. No one thought I was incompetent. Most people seemed relieved to have permission to also not know everything.\u003C/p>\n\u003Cp>The designer who asked me that follow-up question? We ended up having an hour-long conversation where we both admitted we were confused about the same things. We learned together. We actually got it right instead of pretending we already were.\u003C/p>\n\u003Ch2 id=\"an-invitation\">An Invitation\u003C/h2>\n\u003Cp>Next time you’re in a conversation about AI or any new, scary thing try noticing the gap between what you’re saying and what you actually understand.\u003C/p>\n\u003Cp>Notice if you’re reaching for jargon to sound credible. Notice if you’re nodding along when you’re actually lost. Notice if someone else is doing the performance, and maybe offer them an out: “I’m still learning this stuff too, want to figure it out together?”\u003C/p>\n\u003Cp>The shift from “be right” to “get it right” isn’t easy. It won’t feel safe at first.\u003C/p>\n\u003Cp>But I’m finding it’s the only way to actually learn. And maybe, the only way to build real expertise in something that’s changing this fast.\u003C/p>\n\u003Cp>You don’t have to be the expert. You just have to be honest about where you are.\u003C/p>",{"headings":159,"localImagePaths":181,"remoteImagePaths":182,"frontmatter":128,"imagePaths":183},[160,163,166,169,172,175,178],{"depth":34,"slug":161,"text":162},"the-ai-expert-in-the-room-who-doesnt-know-what-a-token-is","The AI Expert in the Room (Who Doesn’t Know What a Token Is)",{"depth":38,"slug":164,"text":165},"were-all-faking-it","We’re All Faking It",{"depth":38,"slug":167,"text":168},"i-did-it-too","I did it too",{"depth":38,"slug":170,"text":171},"the-quote-that-changed-my-pov","The Quote That Changed My POV",{"depth":38,"slug":173,"text":174},"why-this-is-so-hard","Why This Is So Hard",{"depth":38,"slug":176,"text":177},"what-im-trying-now","What I’m Trying Now",{"depth":38,"slug":179,"text":180},"an-invitation","An Invitation",[],[],[],"the-ai-expert-who-doesnt-know-what-a-token-is.md","youre-the-one-who-clicks-submit",{"id":185,"data":187,"body":209,"filePath":210,"digest":211,"rendered":212,"legacyId":236},{"title":188,"date":58,"category":59,"excerpt":189,"takeaways":190},"You're the One Who Clicks Submit","When you use an LLM, you're the driver. Learn why 'good enough' is a trap and how to use a simple testing framework to verify AI outputs before shipping.",[191,194,197,200,203,206],{"title":192,"text":193},"The Temptation to Ship Without Testing","Building an LLM-powered heuristic evaluation tool that looked professional and sounded authoritative, but realizing there was no verification it actually worked beyond one test case. The output looked 'good enough' but that wasn't the same as being reliable.",{"title":195,"text":196},"The Deloitte Wake-Up Call","Deloitte had to refund $400,000 after an AI tool hallucinated in a government report. This highlighted the nightmare scenario: impressive outputs with someone's credibility attached, only to discover later it was wrong.",{"title":198,"text":199},"The 5x5 Testing Protocol","Developed a simple testing methodology: run the same input 5 times to check consistency, then test on 5 different scenarios to verify it generalizes. Not statistically rigorous, but enough to move from 'I hope this works' to 'I've tested it.'",{"title":201,"text":202},"When to Test (and When Not To)","Use rigorous testing when prompts will be reused by others and scalability matters. Skip it for one-off analyses or personal decision-making. Always maintain human-in-the-loop for verification and choose low-risk, reversible use cases.",{"title":204,"text":205},"You Own the Output","When you click submit on LLM-generated work, you're accountable—not the AI. You can't blame the tool when something goes wrong. Before shipping outputs others will trust, you need to verify accuracy and consistency, not just that it 'looks good.'",{"title":207,"text":208},"The Unsolved Challenges","Still uncertain if 5x5 is sufficient for high-stakes scenarios. Difficult to convince fast-moving teams to test when nothing's gone wrong yet. Testing adds friction in a culture that values AI for speed, making it feel like resisting progress rather than professionalism.","# You're the One Who Clicks Submit\n\nMy finger hovered over the \"Copy\" button.\n\nI was staring at Claude's output on my second monitor. The heuristic evaluation looked clean. Professional formatting, proper severity ratings, all ten heuristics addressed. The kind of thing that would make stakeholders nod approvingly in a presentation.\n\nBut I couldn't shake this question: What if someone actually uses this?\n\nSix months earlier, my coworker built something that made me simultaneously impressed and jealous. He created a prompt injection platform with a clever workaround. No RAG capabilities? Just embed the entire knowledge base in the system prompt. The first LLM assistant he created was a dark and deceptive pattern evaluator. Upload UI screenshots, get back analysis of dark and deceptive patterns. The results were undeniably compelling.\n\nThat's when the lightbulb went off: We could do the same thing for heuristic evaluation.\n\nI told him about the idea. He built a prototype. Immediately, I felt that competitive itch: This was my idea and I'm the UX researcher. I have to show this guy I can build something better. (Petty, I know)\n\n## The Problem With \"Good Enough\"\n\nI started with his first draft and began adapting it. Added UXR knowledge from years of running heuristic evaluations myself. Refined the prompt. Tested it on a UI screen I knew had problems. Adjusted the wording. Tested again.\n\nAfter a half-dozen rounds of tweaking, the fifth iteration caught every issue I'd already identified. I looked at the output and thought, This is good enough to ship.\n\nThat's when the real question hit me: But how do I actually know it works?\n\nI'd proven it worked once. On one screen. That I already understood.\n\nBut I was about to hand this to other researchers and designers. Who would use it on designs I'd never seen. And trust the output enough to make decisions based on it.\n\nWhat happens when someone runs this on a different UI? What if they present the findings to stakeholders? What if those stakeholders make product decisions based on what the LLM said?\n\nI realized I had no answer. I'd built something that looked professional, sounded authoritative, and might be completely unreliable.\n\nJust click copy. It's fine. It looks good.\n\nBut that voice saying \"it's fine\" was me being lazy. I had almost convinced myself that I could realize this in the wild. Until I saw what happens when someone skips it.\n\nThe Deloitte headline: $400,000 refund because an AI tool hallucinated. Someone at Deloitte probably had a good day when they submitted that report. Fast turnaround, impressive output. Then came the discovery that it was wrong. With their name on it. With their credibility attached to it.\n\nThat's the nightmare scenario. Not a pulled tool. A $400K mistake.\n\nI was starting to see a common thread from my own work and now the Deloitte headline: someone decided an LLM output was good enough without verifying it actually was.\n\n## Figuring Out Good Enough to Tested\n\nI didn't sit down and design a testing methodology. I just tried answering open questions.\n\nFirst question: Does it give me the same answer with the same inputs?\n\nI ran the same design through the same prompt five times. Not because five is statistically rigorous. It's not. But because it felt like enough to see if a pattern held.\n\nI sat there watching each output generate.\n\nFirst run: identifies contrast issues, navigation problems, three severity-high items.\nSecond run: same issues, different wording.\nThird run: same issues, different wording. Wait… 1 new issue…. Hmm interesting.\nFourth run: back to the same issues, different wording.\nFifth run: same issues, different wording.\n\nFive out of five consistent with some variance. Is that good enough to claim tested? I didn't know. But at least now I knew consistency was a thing I needed to measure. And I had a table stakes approach.\n\nThe outputs varied in wording, obviously. LLMs are probabilistic. But I wasn't looking for identical text. I was checking whether it consistently identified the same usability issues at the same severity levels.\n\nThen the second question emerged: What if I just made a prompt that was hard-coded to find usability issues in the one screen? (I think in the industry they call this over-fitting)\n\nI needed to know if the prompt would scale. So I grabbed five different UI designs I was already familiar with, ones where I knew the usability problems. Ran each through the prompt. Waited for the outputs.\n\nScreen one: caught the issues I expected.\nScreen two: caught those too, plus flagged something I'd missed.\nScreen three: clean identification.\nScreen four: clean.\nScreen five: clean, with one extra insight that actually made sense.\n\nAgain, the \"5\" was arbitrary. It seemed like a good round number. Enough to feel confident without spending a week on testing.\n\nThat moment, watching it work on designs it had never seen, that's when I felt like I could actually hand this to someone else. Not because I'd proven it was perfect. Because I'd verified it was consistent and it generalized beyond the sample I'd tuned it on.\n\nI set the threshold at 5x5: five runs for consistency, five different scenarios for accuracy. Is that sufficient? I still don't know. But it's the line between \"I hope this works\" and \"I've tested it.\"\n\nThat testing protocol felt sufficient for what I was building.\n\n## The Line I'm Still Drawing\n\nSo where's the line? When is testing paranoia versus professionalism?\n\nI use the test when the prompt will be reused and used by others. Scalability is the trigger. If I'm building something that five people, or fifty people, will rely on, I use the 5x5 threshold: five runs for consistency, five different scenarios for accuracy.\n\nBut I don't test everything. One-off prompts? Quick analyses for my own decision-making? I get something good enough and move on. The key difference is who's relying on it and what's at stake if it's wrong.\n\nI also intentionally pick use cases that are low-risk and revertable. Human in the loop, always. I'm not building fully automated systems because I don't trust that I could test them thoroughly enough.\n\nThat's the safety net. Low stakes, easy to fix if something goes sideways, always a person checking the work. Capitalizing on \"human-in-the-loop\" is key.\n\nI'm still not sure if that's enough.\n\n## The Questions I Can't Answer Yet\n\nIs the 5x5 threshold actually sufficient for high-stakes scenarios? It feels right based on my experience, but \"feels right\" isn't exactly rigorous methodology. What if I need 10x10? What if the stakes are high enough that even thorough testing isn't enough? What if I need a large scale fully automated python script testing?\n\nAnd then there's the practical problem: How do I convince other people to test when they just want to ship fast?\n\nI've watched colleagues grab an LLM output, glance at it, and immediately use it in a presentation or send it to a client. When I suggest testing it first, I get the look: Why are you wasting time? It looks good.\n\nIt's hard to argue for caution when nothing's gone wrong yet. The Deloitte story helps, but it feels distant. That was a massive consulting firm, not us. Until something breaks in a visible, costly way, testing feels like paranoia instead of professionalism.\n\nAnd I've gotten questions from peers about losing any efficiency gains because I spend time testing.\n\nThe bigger challenge is scaling this practice across a team that's moving fast. Even if I convince people to test, who has time? We're all under pressure to deliver quickly. Testing adds friction. And in a world where \"AI makes everything faster,\" stopping to verify outputs feels like you're resisting progress.\n\nMy point is this: when you use an LLM to do work, you're still the one accountable for the output.\n\n## You're the Driver\n\nThe AI didn't submit the Deloitte report. A person did. When something generated by an LLM goes wrong, you don't get to say, \"Oops, the AI messed up.\" You're the driver, not the passenger.\n\nBefore you ship an LLM output that other people will trust, you have to be willing to defend it. Not just \"it looks good,\" but \"I've verified it's accurate and consistent.\"\n\nThe person at Deloitte who submitted that hallucinated report probably wishes they'd spent an extra hour testing it.\n\nI'd rather be the person who tests too much than the person who tests too little and learns the hard way.\n\nIf you're building LLM-powered tools, what's your testing threshold? How do you know when \"good enough\" is actually good enough? I'm still figuring this out. If you've found answers I haven't, I want to hear them.","src/content/posts/youre-the-one-who-clicks-submit.md","c9cef456c9fe2003",{"html":213,"metadata":214},"\u003Ch1 id=\"youre-the-one-who-clicks-submit\">You’re the One Who Clicks Submit\u003C/h1>\n\u003Cp>My finger hovered over the “Copy” button.\u003C/p>\n\u003Cp>I was staring at Claude’s output on my second monitor. The heuristic evaluation looked clean. Professional formatting, proper severity ratings, all ten heuristics addressed. The kind of thing that would make stakeholders nod approvingly in a presentation.\u003C/p>\n\u003Cp>But I couldn’t shake this question: What if someone actually uses this?\u003C/p>\n\u003Cp>Six months earlier, my coworker built something that made me simultaneously impressed and jealous. He created a prompt injection platform with a clever workaround. No RAG capabilities? Just embed the entire knowledge base in the system prompt. The first LLM assistant he created was a dark and deceptive pattern evaluator. Upload UI screenshots, get back analysis of dark and deceptive patterns. The results were undeniably compelling.\u003C/p>\n\u003Cp>That’s when the lightbulb went off: We could do the same thing for heuristic evaluation.\u003C/p>\n\u003Cp>I told him about the idea. He built a prototype. Immediately, I felt that competitive itch: This was my idea and I’m the UX researcher. I have to show this guy I can build something better. (Petty, I know)\u003C/p>\n\u003Ch2 id=\"the-problem-with-good-enough\">The Problem With “Good Enough”\u003C/h2>\n\u003Cp>I started with his first draft and began adapting it. Added UXR knowledge from years of running heuristic evaluations myself. Refined the prompt. Tested it on a UI screen I knew had problems. Adjusted the wording. Tested again.\u003C/p>\n\u003Cp>After a half-dozen rounds of tweaking, the fifth iteration caught every issue I’d already identified. I looked at the output and thought, This is good enough to ship.\u003C/p>\n\u003Cp>That’s when the real question hit me: But how do I actually know it works?\u003C/p>\n\u003Cp>I’d proven it worked once. On one screen. That I already understood.\u003C/p>\n\u003Cp>But I was about to hand this to other researchers and designers. Who would use it on designs I’d never seen. And trust the output enough to make decisions based on it.\u003C/p>\n\u003Cp>What happens when someone runs this on a different UI? What if they present the findings to stakeholders? What if those stakeholders make product decisions based on what the LLM said?\u003C/p>\n\u003Cp>I realized I had no answer. I’d built something that looked professional, sounded authoritative, and might be completely unreliable.\u003C/p>\n\u003Cp>Just click copy. It’s fine. It looks good.\u003C/p>\n\u003Cp>But that voice saying “it’s fine” was me being lazy. I had almost convinced myself that I could realize this in the wild. Until I saw what happens when someone skips it.\u003C/p>\n\u003Cp>The Deloitte headline: $400,000 refund because an AI tool hallucinated. Someone at Deloitte probably had a good day when they submitted that report. Fast turnaround, impressive output. Then came the discovery that it was wrong. With their name on it. With their credibility attached to it.\u003C/p>\n\u003Cp>That’s the nightmare scenario. Not a pulled tool. A $400K mistake.\u003C/p>\n\u003Cp>I was starting to see a common thread from my own work and now the Deloitte headline: someone decided an LLM output was good enough without verifying it actually was.\u003C/p>\n\u003Ch2 id=\"figuring-out-good-enough-to-tested\">Figuring Out Good Enough to Tested\u003C/h2>\n\u003Cp>I didn’t sit down and design a testing methodology. I just tried answering open questions.\u003C/p>\n\u003Cp>First question: Does it give me the same answer with the same inputs?\u003C/p>\n\u003Cp>I ran the same design through the same prompt five times. Not because five is statistically rigorous. It’s not. But because it felt like enough to see if a pattern held.\u003C/p>\n\u003Cp>I sat there watching each output generate.\u003C/p>\n\u003Cp>First run: identifies contrast issues, navigation problems, three severity-high items.\nSecond run: same issues, different wording.\nThird run: same issues, different wording. Wait… 1 new issue…. Hmm interesting.\nFourth run: back to the same issues, different wording.\nFifth run: same issues, different wording.\u003C/p>\n\u003Cp>Five out of five consistent with some variance. Is that good enough to claim tested? I didn’t know. But at least now I knew consistency was a thing I needed to measure. And I had a table stakes approach.\u003C/p>\n\u003Cp>The outputs varied in wording, obviously. LLMs are probabilistic. But I wasn’t looking for identical text. I was checking whether it consistently identified the same usability issues at the same severity levels.\u003C/p>\n\u003Cp>Then the second question emerged: What if I just made a prompt that was hard-coded to find usability issues in the one screen? (I think in the industry they call this over-fitting)\u003C/p>\n\u003Cp>I needed to know if the prompt would scale. So I grabbed five different UI designs I was already familiar with, ones where I knew the usability problems. Ran each through the prompt. Waited for the outputs.\u003C/p>\n\u003Cp>Screen one: caught the issues I expected.\nScreen two: caught those too, plus flagged something I’d missed.\nScreen three: clean identification.\nScreen four: clean.\nScreen five: clean, with one extra insight that actually made sense.\u003C/p>\n\u003Cp>Again, the “5” was arbitrary. It seemed like a good round number. Enough to feel confident without spending a week on testing.\u003C/p>\n\u003Cp>That moment, watching it work on designs it had never seen, that’s when I felt like I could actually hand this to someone else. Not because I’d proven it was perfect. Because I’d verified it was consistent and it generalized beyond the sample I’d tuned it on.\u003C/p>\n\u003Cp>I set the threshold at 5x5: five runs for consistency, five different scenarios for accuracy. Is that sufficient? I still don’t know. But it’s the line between “I hope this works” and “I’ve tested it.”\u003C/p>\n\u003Cp>That testing protocol felt sufficient for what I was building.\u003C/p>\n\u003Ch2 id=\"the-line-im-still-drawing\">The Line I’m Still Drawing\u003C/h2>\n\u003Cp>So where’s the line? When is testing paranoia versus professionalism?\u003C/p>\n\u003Cp>I use the test when the prompt will be reused and used by others. Scalability is the trigger. If I’m building something that five people, or fifty people, will rely on, I use the 5x5 threshold: five runs for consistency, five different scenarios for accuracy.\u003C/p>\n\u003Cp>But I don’t test everything. One-off prompts? Quick analyses for my own decision-making? I get something good enough and move on. The key difference is who’s relying on it and what’s at stake if it’s wrong.\u003C/p>\n\u003Cp>I also intentionally pick use cases that are low-risk and revertable. Human in the loop, always. I’m not building fully automated systems because I don’t trust that I could test them thoroughly enough.\u003C/p>\n\u003Cp>That’s the safety net. Low stakes, easy to fix if something goes sideways, always a person checking the work. Capitalizing on “human-in-the-loop” is key.\u003C/p>\n\u003Cp>I’m still not sure if that’s enough.\u003C/p>\n\u003Ch2 id=\"the-questions-i-cant-answer-yet\">The Questions I Can’t Answer Yet\u003C/h2>\n\u003Cp>Is the 5x5 threshold actually sufficient for high-stakes scenarios? It feels right based on my experience, but “feels right” isn’t exactly rigorous methodology. What if I need 10x10? What if the stakes are high enough that even thorough testing isn’t enough? What if I need a large scale fully automated python script testing?\u003C/p>\n\u003Cp>And then there’s the practical problem: How do I convince other people to test when they just want to ship fast?\u003C/p>\n\u003Cp>I’ve watched colleagues grab an LLM output, glance at it, and immediately use it in a presentation or send it to a client. When I suggest testing it first, I get the look: Why are you wasting time? It looks good.\u003C/p>\n\u003Cp>It’s hard to argue for caution when nothing’s gone wrong yet. The Deloitte story helps, but it feels distant. That was a massive consulting firm, not us. Until something breaks in a visible, costly way, testing feels like paranoia instead of professionalism.\u003C/p>\n\u003Cp>And I’ve gotten questions from peers about losing any efficiency gains because I spend time testing.\u003C/p>\n\u003Cp>The bigger challenge is scaling this practice across a team that’s moving fast. Even if I convince people to test, who has time? We’re all under pressure to deliver quickly. Testing adds friction. And in a world where “AI makes everything faster,” stopping to verify outputs feels like you’re resisting progress.\u003C/p>\n\u003Cp>My point is this: when you use an LLM to do work, you’re still the one accountable for the output.\u003C/p>\n\u003Ch2 id=\"youre-the-driver\">You’re the Driver\u003C/h2>\n\u003Cp>The AI didn’t submit the Deloitte report. A person did. When something generated by an LLM goes wrong, you don’t get to say, “Oops, the AI messed up.” You’re the driver, not the passenger.\u003C/p>\n\u003Cp>Before you ship an LLM output that other people will trust, you have to be willing to defend it. Not just “it looks good,” but “I’ve verified it’s accurate and consistent.”\u003C/p>\n\u003Cp>The person at Deloitte who submitted that hallucinated report probably wishes they’d spent an extra hour testing it.\u003C/p>\n\u003Cp>I’d rather be the person who tests too much than the person who tests too little and learns the hard way.\u003C/p>\n\u003Cp>If you’re building LLM-powered tools, what’s your testing threshold? How do you know when “good enough” is actually good enough? I’m still figuring this out. If you’ve found answers I haven’t, I want to hear them.\u003C/p>",{"headings":215,"localImagePaths":233,"remoteImagePaths":234,"frontmatter":187,"imagePaths":235},[216,218,221,224,227,230],{"depth":34,"slug":185,"text":217},"You’re the One Who Clicks Submit",{"depth":38,"slug":219,"text":220},"the-problem-with-good-enough","The Problem With “Good Enough”",{"depth":38,"slug":222,"text":223},"figuring-out-good-enough-to-tested","Figuring Out Good Enough to Tested",{"depth":38,"slug":225,"text":226},"the-line-im-still-drawing","The Line I’m Still Drawing",{"depth":38,"slug":228,"text":229},"the-questions-i-cant-answer-yet","The Questions I Can’t Answer Yet",{"depth":38,"slug":231,"text":232},"youre-the-driver","You’re the Driver",[],[],[],"youre-the-one-who-clicks-submit.md","hello-world",{"id":237,"data":239,"body":250,"filePath":251,"digest":252,"rendered":253,"legacyId":266},{"title":240,"date":58,"category":241,"excerpt":242,"takeaways":243},"Hello World","General","This is a sample post written in Markdown.",[244,247],{"title":245,"text":246},"Markdown is Easy","You can write complex posts using simple text syntax.",{"title":248,"text":249},"Dual Modes","Readers can switch between deep reading and quick skimming.","# Welcome to the Blog\n\nThis is a sample post to demonstrate the **Markdown** capabilities.\n\n## Features\n- Write in standard Markdown\n- Add images: ![Placeholder](/vite.svg)\n- Code blocks:\n\n```javascript\nconsole.log(\"Hello World\");\n```\n\nEnjoy writing!","src/content/posts/hello-world.md","d91b25c1471de14b",{"html":254,"metadata":255},"\u003Ch1 id=\"welcome-to-the-blog\">Welcome to the Blog\u003C/h1>\n\u003Cp>This is a sample post to demonstrate the \u003Cstrong>Markdown\u003C/strong> capabilities.\u003C/p>\n\u003Ch2 id=\"features\">Features\u003C/h2>\n\u003Cul>\n\u003Cli>Write in standard Markdown\u003C/li>\n\u003Cli>Add images: \u003Cimg src=\"/vite.svg\" alt=\"Placeholder\">\u003C/li>\n\u003Cli>Code blocks:\u003C/li>\n\u003C/ul>\n\u003Cpre class=\"astro-code github-dark\" style=\"background-color:#24292e;color:#e1e4e8; overflow-x: auto;\" tabindex=\"0\" data-language=\"javascript\">\u003Ccode>\u003Cspan class=\"line\">\u003Cspan style=\"color:#E1E4E8\">console.\u003C/span>\u003Cspan style=\"color:#B392F0\">log\u003C/span>\u003Cspan style=\"color:#E1E4E8\">(\u003C/span>\u003Cspan style=\"color:#9ECBFF\">\"Hello World\"\u003C/span>\u003Cspan style=\"color:#E1E4E8\">);\u003C/span>\u003C/span>\u003C/code>\u003C/pre>\n\u003Cp>Enjoy writing!\u003C/p>",{"headings":256,"localImagePaths":263,"remoteImagePaths":264,"frontmatter":239,"imagePaths":265},[257,260],{"depth":34,"slug":258,"text":259},"welcome-to-the-blog","Welcome to the Blog",{"depth":38,"slug":261,"text":262},"features","Features",[],[],[],"hello-world.md"]